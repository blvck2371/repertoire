# Option B : develop = pipeline complète (lint, test, scan, E2E) ; preprod/prod = Build → Push Harbor → Deploy uniquement
name: Frontend CI

on:
  push:
    branches: [develop, preprod, prod]
  pull_request:
    branches: [develop, preprod, prod]
  workflow_dispatch:
    inputs:
      ref:
        description: 'Branche à exécuter (défaut: branche actuelle)'
        required: false

permissions:
  contents: write
  actions: write   # requis pour déclencher workflow_dispatch via API

jobs:
  # 1. Validation : lint+test sur develop/PR, no-op sur preprod/prod (toujours exécuté pour débloquer le build)
  frontend-validation:
    name: Validation (lint+test)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: Lint
        if: github.ref_name == 'develop' || github.event_name == 'pull_request'
        run: cd frontend && npm run lint

      - name: Test
        if: github.ref_name == 'develop' || github.event_name == 'pull_request'
        run: cd frontend && npm run test

      - name: Skip (preprod/prod)
        if: github.ref_name != 'develop' && github.event_name != 'pull_request'
        run: echo "Validation déjà faite sur develop"

  # 2. Build npm (production)
  frontend-build:
    name: Build npm
    runs-on: ubuntu-latest
    needs: frontend-validation
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: cd frontend && npm ci

      - name: Build
        run: cd frontend && npm run build

  # 4. Build image Docker
  frontend-docker:
    name: Build Docker
    runs-on: ubuntu-latest
    needs: frontend-build
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build image
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: false
          load: true
          tags: repertoire-frontend:test
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Save image
        run: docker save repertoire-frontend:test | gzip > /tmp/frontend.tar.gz

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: frontend-image
          path: /tmp/frontend.tar.gz

  # 5. Scan sécurité (Trivy) – develop/PR uniquement
  frontend-scan:
    name: Scan Trivy
    runs-on: ubuntu-latest
    needs: frontend-docker
    if: github.ref_name == 'develop' || github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4

      - name: Download image
        uses: actions/download-artifact@v4
        with:
          name: frontend-image

      - name: Load image
        run: |
          IMG=$(find . -name "*.tar.gz" -type f | head -1)
          gunzip -c "$IMG" | docker load

      - name: Scan
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'repertoire-frontend:test'
          format: 'table'
          exit-code: '0'

  # 6. Tests E2E – develop/PR uniquement
  frontend-e2e:
    name: E2E
    runs-on: ubuntu-latest
    needs: frontend-docker
    if: github.ref_name == 'develop' || github.event_name == 'pull_request'
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build backend image (for E2E)
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          push: false
          load: true
          tags: repertoire-backend:test
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Download frontend image
        uses: actions/download-artifact@v4
        with:
          name: frontend-image

      - name: Load frontend image
        run: |
          IMG=$(find . -name "*.tar.gz" -type f | head -1)
          gunzip -c "$IMG" | docker load

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-chromium-${{ hashFiles('package-lock.json') }}

      - name: Install Playwright
        run: npm ci && npx playwright install --with-deps chromium

      - name: Start stack
        run: docker compose -f docker/backend/docker-compose.yml -f docker/frontend/docker-compose.yml --project-directory . up -d

      - name: Wait for backend
        run: |
          for i in $(seq 1 60); do
            curl -sf http://localhost/api/health && echo "Backend ready" && break
            sleep 2
          done

      - name: Run E2E tests
        run: npx playwright test --project=chromium

      - name: Stop stack
        if: always()
        run: docker compose -f docker/backend/docker-compose.yml -f docker/frontend/docker-compose.yml --project-directory . down -v

  # 7. CD - Push vers Harbor (push ou workflow_dispatch, si HARBOR_ENABLED)
  frontend-cd:
    name: CD (Push Harbor)
    runs-on: ubuntu-latest
    needs: frontend-docker
    if: (github.event_name == 'push' || github.event_name == 'workflow_dispatch') && vars.HARBOR_ENABLED == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Download image
        uses: actions/download-artifact@v4
        with:
          name: frontend-image

      - name: Load image
        run: |
          IMG=$(find . -name "*.tar.gz" -type f | head -1)
          gunzip -c "$IMG" | docker load

      - name: Set Harbor project
        id: harbor
        run: |
          case "${{ github.ref_name }}" in
            develop) PROJECT="dev" ;;
            preprod) PROJECT="preprod" ;;
            prod)    PROJECT="prod" ;;
            *)       PROJECT="dev" ;;
          esac
          echo "project=$PROJECT" >> $GITHUB_OUTPUT
          echo "tag=${{ github.ref_name }}-${GITHUB_SHA:0:7}" >> $GITHUB_OUTPUT

      - name: Configure Docker for Harbor (HTTP ou HTTPS auto-signé)
        if: vars.HARBOR_HTTPS != 'true'
        run: |
          echo '{"insecure-registries":["${{ secrets.HARBOR_URL }}"]}' | sudo tee /etc/docker/daemon.json
          sudo systemctl restart docker

      - name: Login to Harbor
        uses: docker/login-action@v3
        with:
          registry: ${{ secrets.HARBOR_URL }}
          username: ${{ secrets.HARBOR_USERNAME }}
          password: ${{ secrets.HARBOR_PASSWORD }}

      - name: Push to Harbor
        run: |
          docker tag repertoire-frontend:test ${{ secrets.HARBOR_URL }}/${{ steps.harbor.outputs.project }}/repertoire-frontend:${{ steps.harbor.outputs.tag }}
          docker tag repertoire-frontend:test ${{ secrets.HARBOR_URL }}/${{ steps.harbor.outputs.project }}/repertoire-frontend:latest
          docker push ${{ secrets.HARBOR_URL }}/${{ steps.harbor.outputs.project }}/repertoire-frontend:${{ steps.harbor.outputs.tag }}
          docker push ${{ secrets.HARBOR_URL }}/${{ steps.harbor.outputs.project }}/repertoire-frontend:latest

  # 8a. Deploy Infra - Traefik, cert-manager, Prometheus, Grafana
  deploy-infra:
    name: Deploy Infra
    runs-on: ubuntu-latest
    needs: frontend-cd
    if: (github.event_name == 'push' || github.event_name == 'workflow_dispatch') && vars.HARBOR_ENABLED == 'true' && vars.KUBERNETES_DEPLOY_ENABLED == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.13.0'

      - name: Configure kubectl
        env:
          KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
        run: |
          mkdir -p ~/.kube
          # Support base64 ou texte brut (copier-coller du kubeconfig)
          if echo "$KUBE_CONFIG" | tr -d '\n\r ' | base64 -d > ~/.kube/config 2>/dev/null && [ -s ~/.kube/config ]; then
            echo "Kubeconfig décodé (base64)"
          else
            printf '%s' "$KUBE_CONFIG" > ~/.kube/config
            echo "Kubeconfig utilisé en texte brut"
          fi
          chmod 600 ~/.kube/config

      - name: Set env from branch
        id: env
        run: |
          case "${{ github.ref_name }}" in
            develop) NS=repertoire-dev; HOST=repertoire-app.duckdns.org; VALS=values-dev.yaml ;;
            preprod) NS=repertoire-preprod; HOST=repertoire-preprod.duckdns.org; VALS=values-preprod.yaml ;;
            prod)    NS=repertoire-prod; HOST=repertoire-prod.duckdns.org; VALS=values-prod.yaml ;;
            *)      NS=repertoire-dev; HOST=repertoire-app.duckdns.org; VALS=values-dev.yaml ;;
          esac
          echo "namespace=$NS" >> $GITHUB_OUTPUT
          echo "host=$HOST" >> $GITHUB_OUTPUT
          echo "values=$VALS" >> $GITHUB_OUTPUT

      - name: Create namespace if not exists
        run: |
          kubectl create namespace ${{ steps.env.outputs.namespace }} --dry-run=client -o yaml | kubectl apply -f -
          kubectl create namespace repertoire-dev --dry-run=client -o yaml | kubectl apply -f -

      - name: Create Harbor secret if not exists
        run: |
          kubectl create secret docker-registry harbor-creds \
            --docker-server=${{ secrets.HARBOR_URL }} \
            --docker-username=${{ secrets.HARBOR_USERNAME }} \
            --docker-password=${{ secrets.HARBOR_PASSWORD }} \
            -n ${{ steps.env.outputs.namespace }} \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Install Traefik (Ingress controller)
        run: |
          helm repo add traefik https://traefik.github.io/charts --force-update
          helm repo update
          kubectl create namespace traefik --dry-run=client -o yaml | kubectl apply -f -
          helm upgrade --install traefik traefik/traefik \
            -n traefik \
            --set deployment.replicas=1 \
            --wait --timeout 2m || true

      - name: Install cert-manager (Let's Encrypt)
        run: |
          helm repo add jetstack https://charts.jetstack.io --force-update
          helm repo update
          kubectl create namespace cert-manager --dry-run=client -o yaml | kubectl apply -f -
          helm upgrade --install cert-manager jetstack/cert-manager \
            -n cert-manager \
            --set installCRDs=true \
            --wait --timeout 2m || true

      - name: Create Let's Encrypt ClusterIssuer
        env:
          LE_EMAIL: ${{ vars.LETSENCRYPT_EMAIL }}
        run: |
          EMAIL="${LE_EMAIL:-thetiptopprojectgroup@gmail.com}"
          kubectl apply -f - <<EOF
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt-prod
          spec:
            acme:
              server: https://acme-v02.api.letsencrypt.org/directory
              email: ${EMAIL}
              privateKeySecretRef:
                name: letsencrypt-prod
              solvers:
                - http01:
                    ingress:
                      class: traefik
          EOF

      - name: Create Certificate (cert-manager, hors Helm)
        run: |
          kubectl delete certificate repertoire-tls -n ${{ steps.env.outputs.namespace }} --ignore-not-found=true --wait=false 2>/dev/null || true
          sleep 3
          kubectl apply -f - <<EOF
          apiVersion: cert-manager.io/v1
          kind: Certificate
          metadata:
            name: repertoire-tls
            namespace: ${{ steps.env.outputs.namespace }}
          spec:
            secretName: repertoire-tls
            issuerRef:
              name: letsencrypt-prod
              kind: ClusterIssuer
            dnsNames:
              - ${{ steps.env.outputs.host }}
          EOF

      - name: Install Prometheus + Grafana (Phase 8)
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts --force-update
          helm repo update
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            --set prometheus.prometheusSpec.retention=7d \
            --set grafana.adminPassword=repertoire-monitoring \
            --set grafana.persistence.enabled=false \
            --wait --timeout 5m || true

      - name: Appliquer règles Prometheus (Phase 8)
        run: |
          kubectl apply -f helm/repertoire/monitoring/prometheus-rules.yaml

      - name: Configurer Alertmanager (Slack, si webhook fourni)
        env:
          SLACK_WEBHOOK: ${{ secrets.ALERTMANAGER_SLACK_WEBHOOK }}
        run: |
          if [[ -n "$SLACK_WEBHOOK" ]]; then
            sed "s|SLACK_WEBHOOK_PLACEHOLDER|$SLACK_WEBHOOK|g" helm/repertoire/monitoring/alertmanager-config.yaml > /tmp/alertmanager.yaml
            kubectl create secret generic alertmanager-repertoire-config -n monitoring \
              --from-file=alertmanager.yaml=/tmp/alertmanager.yaml \
              --dry-run=client -o yaml | kubectl apply -f -
            helm upgrade kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              -n monitoring \
              --reuse-values \
              --set alertmanager.alertmanagerSpec.useExistingSecret=true \
              --set alertmanager.alertmanagerSpec.configSecret=alertmanager-repertoire-config \
              --wait --timeout 2m || true
          fi

      - name: Exposer Grafana (IngressRoute)
        run: |
          kubectl delete certificate grafana-tls -n monitoring --ignore-not-found=true --wait=false 2>/dev/null || true
          sleep 2
          kubectl apply -f - <<'GRAFANAEOF'
          apiVersion: cert-manager.io/v1
          kind: Certificate
          metadata:
            name: grafana-tls
            namespace: monitoring
          spec:
            secretName: grafana-tls
            issuerRef:
              name: letsencrypt-prod
              kind: ClusterIssuer
            dnsNames:
              - grafana-repertoire.duckdns.org
          ---
          apiVersion: traefik.io/v1alpha1
          kind: IngressRoute
          metadata:
            name: grafana
            namespace: monitoring
          spec:
            entryPoints:
              - web
              - websecure
            routes:
              - match: Host(`grafana-repertoire.duckdns.org`) && PathPrefix(`/`)
                kind: Rule
                services:
                  - name: kube-prometheus-stack-grafana
                    port: 80
            tls:
              secretName: grafana-tls
          GRAFANAEOF

      - name: Install Vault (Phase 7, si VAULT_ENABLED)
        if: vars.VAULT_ENABLED == 'true'
        run: |
          helm repo add hashicorp https://helm.releases.hashicorp.com --force-update
          helm repo update
          kubectl create namespace vault --dry-run=client -o yaml | kubectl apply -f -
          kubectl apply -f helm/repertoire/vault/vault-rbac.yaml
          helm upgrade --install vault hashicorp/vault \
            -n vault \
            -f helm/repertoire/vault/vault-values.yaml \
            --wait --timeout 3m || true

      - name: Init Vault (Phase 7 – KV, Kubernetes auth, secret MongoDB)
        if: vars.VAULT_ENABLED == 'true'
        run: |
          kubectl delete job vault-init -n vault --ignore-not-found=true --wait=false 2>/dev/null || true
          sleep 3
          kubectl apply -f helm/repertoire/vault/vault-init-job.yaml
          kubectl wait --for=condition=complete job/vault-init -n vault --timeout=120s || true

  # 8b. Deploy Backup – MinIO + Restic (Phase 9, develop uniquement)
  deploy-backup:
    name: Deploy Backup (MinIO + Restic)
    runs-on: ubuntu-latest
    needs: deploy-infra
    if: (github.event_name == 'push' || github.event_name == 'workflow_dispatch') && vars.HARBOR_ENABLED == 'true' && vars.KUBERNETES_DEPLOY_ENABLED == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.13.0'

      - name: Configure kubectl
        env:
          KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
        run: |
          mkdir -p ~/.kube
          if echo "$KUBE_CONFIG" | tr -d '\n\r ' | base64 -d > ~/.kube/config 2>/dev/null && [ -s ~/.kube/config ]; then
            echo "Kubeconfig décodé (base64)"
          else
            printf '%s' "$KUBE_CONFIG" > ~/.kube/config
          fi
          chmod 600 ~/.kube/config

      - name: Login to Harbor
        uses: docker/login-action@v3
        with:
          registry: ${{ secrets.HARBOR_URL }}
          username: ${{ secrets.HARBOR_USERNAME }}
          password: ${{ secrets.HARBOR_PASSWORD }}

      - name: Build et push image backup
        run: |
          docker build -t repertoire-backup:latest -f docker/backup/Dockerfile docker/backup
          docker tag repertoire-backup:latest ${{ secrets.HARBOR_URL }}/dev/repertoire-backup:latest
          docker push ${{ secrets.HARBOR_URL }}/dev/repertoire-backup:latest

      - name: Deploy MinIO (image officielle, sans Helm)
        run: |
          helm uninstall repertoire-minio -n repertoire-dev --ignore-not-found=true 2>/dev/null || true
          sleep 5
          kubectl apply -f helm/repertoire/minio/minio-deployment.yaml
          kubectl rollout status deployment/repertoire-minio -n repertoire-dev --timeout=2m
      - name: Créer bucket repertoire-backups
        run: |
          kubectl delete job minio-init-bucket -n repertoire-dev --ignore-not-found=true
          kubectl apply -f - <<'BUCKET'
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: minio-init-bucket
            namespace: repertoire-dev
          spec:
            ttlSecondsAfterFinished: 120
            template:
              spec:
                restartPolicy: OnFailure
                containers:
                  - name: mc
                    image: minio/mc:latest
                    command:
                      - /bin/sh
                      - -c
                      - |
                        sleep 10
                        mc alias set m http://repertoire-minio:9000 minioadmin minioadmin
                        mc mb m/repertoire-backups --ignore-existing
                        echo Bucket OK
          BUCKET
          kubectl wait --for=condition=complete job/minio-init-bucket -n repertoire-dev --timeout=90s || true

      - name: Create backup credentials secret
        run: |
          kubectl create secret generic repertoire-backup-credentials -n repertoire-dev \
            --from-literal=minio-access-key=minioadmin \
            --from-literal=minio-secret-key=minioadmin \
            --from-literal=restic-password=repertoire-backup-dev \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Exposer MinIO Console et API en HTTPS (IngressRoute)
        run: |
          kubectl delete certificate minio-tls -n repertoire-dev --ignore-not-found=true --wait=false 2>/dev/null || true
          kubectl delete certificate minio-api-tls -n repertoire-dev --ignore-not-found=true --wait=false 2>/dev/null || true
          sleep 2
          kubectl apply -f - <<'MINIOEOF'
          apiVersion: cert-manager.io/v1
          kind: Certificate
          metadata:
            name: minio-tls
            namespace: repertoire-dev
          spec:
            secretName: minio-tls
            issuerRef:
              name: letsencrypt-prod
              kind: ClusterIssuer
            dnsNames:
              - minio-repertoire.duckdns.org
          ---
          apiVersion: cert-manager.io/v1
          kind: Certificate
          metadata:
            name: minio-api-tls
            namespace: repertoire-dev
          spec:
            secretName: minio-api-tls
            issuerRef:
              name: letsencrypt-prod
              kind: ClusterIssuer
            dnsNames:
              - minio-api-repertoire.duckdns.org
          ---
          apiVersion: traefik.io/v1alpha1
          kind: IngressRoute
          metadata:
            name: minio-console
            namespace: repertoire-dev
          spec:
            entryPoints:
              - web
              - websecure
            routes:
              - match: Host(`minio-repertoire.duckdns.org`) && PathPrefix(`/`)
                kind: Rule
                services:
                  - name: repertoire-minio
                    port: 9001
            tls:
              secretName: minio-tls
          ---
          apiVersion: traefik.io/v1alpha1
          kind: IngressRoute
          metadata:
            name: minio-api
            namespace: repertoire-dev
          spec:
            entryPoints:
              - web
              - websecure
            routes:
              - match: Host(`minio-api-repertoire.duckdns.org`) && PathPrefix(`/`)
                kind: Rule
                services:
                  - name: repertoire-minio
                    port: 9000
            tls:
              secretName: minio-api-tls
          MINIOEOF

      - name: Attendre les certificats MinIO (Let's Encrypt)
        run: sleep 45

  # 8c. Deploy App – Helm repertoire
  deploy:
    name: Deploy K8s
    runs-on: ubuntu-latest
    needs: [deploy-infra, deploy-backup]
    if: (github.event_name == 'push' || github.event_name == 'workflow_dispatch') && vars.HARBOR_ENABLED == 'true' && vars.KUBERNETES_DEPLOY_ENABLED == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.13.0'

      - name: Configure kubectl
        env:
          KUBE_CONFIG: ${{ secrets.KUBE_CONFIG }}
        run: |
          mkdir -p ~/.kube
          if echo "$KUBE_CONFIG" | tr -d '\n\r ' | base64 -d > ~/.kube/config 2>/dev/null && [ -s ~/.kube/config ]; then
            echo "Kubeconfig décodé (base64)"
          else
            printf '%s' "$KUBE_CONFIG" > ~/.kube/config
          fi
          chmod 600 ~/.kube/config

      - name: Set env from branch
        id: env
        run: |
          case "${{ github.ref_name }}" in
            develop) NS=repertoire-dev; VALS=values-dev.yaml ;;
            preprod) NS=repertoire-preprod; VALS=values-preprod.yaml ;;
            prod)    NS=repertoire-prod; VALS=values-prod.yaml ;;
            *)      NS=repertoire-dev; VALS=values-dev.yaml ;;
          esac
          echo "namespace=$NS" >> $GITHUB_OUTPUT
          echo "values=$VALS" >> $GITHUB_OUTPUT

      - name: Nettoyer MongoDB (Pod/StatefulSet bloqués)
        run: |
          NS=${{ steps.env.outputs.namespace }}
          kubectl delete pod -l app.kubernetes.io/component=mongodb -n $NS --ignore-not-found=true --force --grace-period=0 2>/dev/null || true
          kubectl delete statefulset repertoire-mongodb -n $NS --ignore-not-found=true --wait=false 2>/dev/null || true
          # Ne jamais supprimer le PVC en preprod/prod = les données MongoDB sont conservées
          # (dev utilise emptyDir, pas de PVC)
          sleep 10

      - name: Deploy / Upgrade Helm
        run: |
          VAULT_SET=""
          [[ "${{ vars.VAULT_ENABLED }}" == "true" ]] && VAULT_SET="--set vault.enabled=true"
          helm upgrade --install repertoire ./helm/repertoire \
            -n ${{ steps.env.outputs.namespace }} \
            -f helm/repertoire/values.yaml \
            -f helm/repertoire/${{ steps.env.outputs.values }} \
            $VAULT_SET \
            --timeout 3m
          echo "Helm deploy terminé, attente des pods..."

      - name: Force MongoDB pod refresh (si bloqué)
        run: |
          kubectl delete pod -l app.kubernetes.io/component=mongodb -n ${{ steps.env.outputs.namespace }} --ignore-not-found=true || true
          sleep 10

      - name: Wait for MongoDB
        run: |
          kubectl rollout status statefulset/repertoire-mongodb -n ${{ steps.env.outputs.namespace }} --timeout=5m

      - name: Wait for Backend and Frontend
        run: |
          kubectl rollout status deployment/repertoire-backend -n ${{ steps.env.outputs.namespace }} --timeout=5m
          kubectl rollout status deployment/repertoire-frontend -n ${{ steps.env.outputs.namespace }} --timeout=5m

      - name: Debug pods (si échec)
        if: failure()
        run: |
          echo "=== Pods ==="
          kubectl get pods -n ${{ steps.env.outputs.namespace }} -o wide
          echo "=== PVC ==="
          kubectl get pvc -n ${{ steps.env.outputs.namespace }}
          echo "=== Describe MongoDB pod (si Pending) ==="
          kubectl describe pod -l app.kubernetes.io/component=mongodb -n ${{ steps.env.outputs.namespace }} 2>/dev/null | tail -50 || true
          echo "=== Events ==="
          kubectl get events -n ${{ steps.env.outputs.namespace }} --sort-by='.lastTimestamp' | tail -30

      - name: Restart deployments (pull latest images)
        run: |
          kubectl rollout restart deployment -n ${{ steps.env.outputs.namespace }} -l app.kubernetes.io/instance=repertoire
          kubectl rollout status deployment -n ${{ steps.env.outputs.namespace }} -l app.kubernetes.io/instance=repertoire --timeout=5m

  # 8d. Auto-merge : develop → preprod → prod (si pipeline OK)
  # Sur preprod : s'exécute aussi en workflow_dispatch (car preprod est déclenché par API, pas par push)
  auto-merge:
    name: Auto-merge develop→preprod→prod
    runs-on: ubuntu-latest
    needs: [deploy, frontend-build]
    if: |
      github.ref_name != 'prod' &&
      (needs.deploy.result == 'success' || needs.deploy.result == 'skipped') &&
      (github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.ref_name == 'preprod'))
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Push develop → preprod
        if: github.ref_name == 'develop'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git push origin HEAD:preprod
          sleep 3

      - name: Déclencher pipelines preprod (frontend + backend)
        if: github.ref_name == 'develop'
        run: |
          for w in frontend-ci.yml backend-ci.yml; do
            HTTP=$(curl -sS -w "%{http_code}" -o /tmp/curl_out -X POST \
              -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
              -H "Accept: application/vnd.github+json" \
              -H "X-GitHub-Api-Version: 2022-11-28" \
              "https://api.github.com/repos/${{ github.repository }}/actions/workflows/$w/dispatches" \
              -d '{"ref":"preprod"}')
            if [[ "$HTTP" != "204" && "$HTTP" != "200" ]]; then
              echo "::error::Échec déclenchement $w (HTTP $HTTP): $(cat /tmp/curl_out)"
              exit 1
            fi
            echo "Déclenché: $w sur preprod (HTTP $HTTP)"
          done

      - name: Push preprod → prod
        if: github.ref_name == 'preprod'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          # --force : prod doit refléter exactement preprod (évite "non fast forward" si prod a divergé)
          git push origin HEAD:prod --force
          sleep 3

      - name: Déclencher pipelines prod (frontend + backend)
        if: github.ref_name == 'preprod'
        run: |
          for w in frontend-ci.yml backend-ci.yml; do
            HTTP=$(curl -sS -w "%{http_code}" -o /tmp/curl_out -X POST \
              -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
              -H "Accept: application/vnd.github+json" \
              -H "X-GitHub-Api-Version: 2022-11-28" \
              "https://api.github.com/repos/${{ github.repository }}/actions/workflows/$w/dispatches" \
              -d '{"ref":"prod"}')
            if [[ "$HTTP" != "204" && "$HTTP" != "200" ]]; then
              echo "::error::Échec déclenchement $w (HTTP $HTTP): $(cat /tmp/curl_out)"
              exit 1
            fi
            echo "Déclenché: $w sur prod (HTTP $HTTP)"
          done

  # 9. Notifications (Phase 2) – Slack ou Discord
  notify:
    name: Notify
    runs-on: ubuntu-latest
    needs: [frontend-validation, frontend-build, frontend-docker, frontend-scan, frontend-e2e]
    if: always() && (needs.frontend-scan.result == 'success' || needs.frontend-scan.result == 'skipped') && (needs.frontend-e2e.result == 'success' || needs.frontend-e2e.result == 'skipped')
    steps:
      - name: Déterminer le statut
        id: status
        run: |
          FAILED=false
          for r in "${{ needs.frontend-validation.result }}" "${{ needs.frontend-build.result }}" "${{ needs.frontend-docker.result }}" "${{ needs.frontend-scan.result }}" "${{ needs.frontend-e2e.result }}"; do
            [[ "$r" == "failure" ]] && FAILED=true && break
          done
          if [[ "$FAILED" == "true" ]]; then
            echo "status=ÉCHEC" >> $GITHUB_OUTPUT
            echo "emoji=❌" >> $GITHUB_OUTPUT
            echo "color=15158332" >> $GITHUB_OUTPUT
          else
            echo "status=Succès" >> $GITHUB_OUTPUT
            echo "emoji=✅" >> $GITHUB_OUTPUT
            echo "color=3066993" >> $GITHUB_OUTPUT
          fi

      - name: Notifier Slack
        run: |
          [[ -z "${{ secrets.SLACK_WEBHOOK_URL }}" ]] && exit 0
          curl -sS -X POST -H 'Content-type: application/json' \
            --data '{"text":"${{ steps.status.outputs.emoji }} *Frontend CI* – ${{ steps.status.outputs.status }}\nBranche: `${{ github.ref_name }}` | Commit: `${{ github.sha }}`\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|Voir le workflow>"}' \
            "${{ secrets.SLACK_WEBHOOK_URL }}" || true

      - name: Notifier Discord
        run: |
          [[ -z "${{ secrets.DISCORD_WEBHOOK_URL }}" ]] && exit 0
          curl -sS -X POST -H 'Content-type: application/json' \
            --data '{"embeds":[{"title":"${{ steps.status.outputs.emoji }} Frontend CI – ${{ steps.status.outputs.status }}","color":${{ steps.status.outputs.color }},"fields":[{"name":"Branche","value":"`${{ github.ref_name }}`","inline":true},{"name":"Commit","value":"`${{ github.sha }}`","inline":true},{"name":"Workflow","value":"[Voir](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})","inline":false}]}]}' \
            "${{ secrets.DISCORD_WEBHOOK_URL }}" || true


